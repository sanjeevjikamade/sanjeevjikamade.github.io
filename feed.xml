

<feed xmlns="http://www.w3.org/2005/Atom">
  <id>https://sanjeevjikamade.github.io/</id>
  <title>Sanjeev Jikamade</title>
  <subtitle>Sanjeev, Blog, Github, Machine Learning, Python</subtitle>
  <updated>2025-06-28T09:04:21+05:30</updated>
  <author>
    <name>Sanjeev Jikamade</name>
    <uri>https://sanjeevjikamade.github.io/</uri>
  </author>
  <link rel="self" type="application/atom+xml" href="https://sanjeevjikamade.github.io/feed.xml"/>
  <link rel="alternate" type="text/html" hreflang="en"
    href="https://sanjeevjikamade.github.io/"/>
  <generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator>
  <rights> © 2025 Sanjeev Jikamade </rights>
  <icon>/assets/img/favicons/favicon.ico</icon>
  <logo>/assets/img/favicons/favicon-96x96.png</logo>


  
  <entry>
    <title>Spegel - Stateless Local OCI Mirror</title>
    <link href="https://sanjeevjikamade.github.io/posts/Spegel-stateless-local-OCI-mirror/" rel="alternate" type="text/html" title="Spegel - Stateless Local OCI Mirror" />
    <published>2024-11-04T00:00:00+05:30</published>
  
    <updated>2024-11-04T00:00:00+05:30</updated>
  
    <id>https://sanjeevjikamade.github.io/posts/Spegel-stateless-local-OCI-mirror/</id>
    <content src="https://sanjeevjikamade.github.io/posts/Spegel-stateless-local-OCI-mirror/" />
    <author>
      <name>tremo</name>
    </author>

  
    
    <category term="OCI" />
    
    <category term="Mirror" />
    
    <category term="Spegel" />
    
  

  
    <summary>
      





      Spegel describes itself as a stateless, cluster-local OCI registry mirror. In this post, we’ll decipher what that means and explore why Spegel might be valuable for your Kubernetes (K8s) setup.

What is Spegel?


  Stateless: Spegel doesn’t store image manifests or layers itself; it relies on containerd’s store on the node.
  Local: It operates within your local K8s cluster.
  OCI Registry: It’...
    </summary>
  

  </entry>

  
  <entry>
    <title>How to Efficiently Serve an LLM?</title>
    <link href="https://sanjeevjikamade.github.io/posts/How-to-Efficiently-serve-an-llm/" rel="alternate" type="text/html" title="How to Efficiently Serve an LLM?" />
    <published>2024-08-05T05:30:00+05:30</published>
  
    <updated>2024-08-05T05:30:00+05:30</updated>
  
    <id>https://sanjeevjikamade.github.io/posts/How-to-Efficiently-serve-an-llm/</id>
    <content src="https://sanjeevjikamade.github.io/posts/How-to-Efficiently-serve-an-llm/" />
    <author>
      <name>tremo</name>
    </author>

  
    
    <category term="LLM" />
    
    <category term="Inference" />
    
    <category term="Optimization" />
    
    <category term="Serving" />
    
  

  
    <summary>
      





      How to Efficiently Serve an LLM

LLMs, or Large Language Models, are named so because they can range from tens to hundreds of billions of parameters. Their utility is clear, as LLMs are setting new records on various benchmarks and now often match or exceed human performance in multiple tasks GPT-4 Technical Report. Consequently, many companies are eager to deploy them in production. However, d...
    </summary>
  

  </entry>

  
  <entry>
    <title>The Tech Behind TikTok's Addictive Recommendation System</title>
    <link href="https://sanjeevjikamade.github.io/posts/the-tech-behind-tiktoks-addicitve-recommendation-system/" rel="alternate" type="text/html" title="The Tech Behind TikTok&amp;apos;s Addictive Recommendation System" />
    <published>2023-12-05T01:00:00+05:30</published>
  
    <updated>2023-12-05T01:00:00+05:30</updated>
  
    <id>https://sanjeevjikamade.github.io/posts/the-tech-behind-tiktoks-addicitve-recommendation-system/</id>
    <content src="https://sanjeevjikamade.github.io/posts/the-tech-behind-tiktoks-addicitve-recommendation-system/" />
    <author>
      <name>tremo</name>
    </author>

  
    
    <category term="TikTok" />
    
    <category term="Recommendation Systems" />
    
    <category term="Kafka" />
    
    <category term="Flink" />
    
  

  
    <summary>
      





      Intro

I’ve been using TikTok a lot lately and was curious about the tech behind TikTok’s addictive algorithm, the “Recommendation System.” I visited their official blog post but found limited details on how it works. So, I googled and found two papers “Monolith” and “IPS,” released by ByteDance, TikTok’s parent company, and here is the process as explained by them.

Recommendation features

Th...
    </summary>
  

  </entry>

  
  <entry>
    <title>Do you really need a Vector Database?</title>
    <link href="https://sanjeevjikamade.github.io/posts/do-you-really-need-a-vector-database/" rel="alternate" type="text/html" title="Do you really need a Vector Database?" />
    <published>2023-11-22T02:20:00+05:30</published>
  
    <updated>2023-11-22T02:20:00+05:30</updated>
  
    <id>https://sanjeevjikamade.github.io/posts/do-you-really-need-a-vector-database/</id>
    <content src="https://sanjeevjikamade.github.io/posts/do-you-really-need-a-vector-database/" />
    <author>
      <name>tremo</name>
    </author>

  
    
    <category term="LLMs" />
    
    <category term="GenAI" />
    
    <category term="VectorDBs" />
    
    <category term="Embeddings" />
    
  

  
    <summary>
      





      Do You Really Need a VectorDB?

Everywhere I go, I see his face – “VectorDBs”. Seriously, with the rise of GenAI and the growing number of applications utilizing Large Language Models (LLMs), Vector Databases have been posited as a de facto component in any LLM-powered application architecture. You might wonder why. The answer is seemingly straightforward: “LLMs have a limited context window, a...
    </summary>
  

  </entry>

</feed>



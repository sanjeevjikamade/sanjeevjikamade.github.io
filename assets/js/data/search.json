[
  
  {
    "title": "YouTube is Free University for AI/ML Engineers",
    "url": "/posts/YouTube-is-Free-University-for-AIML-Engineers/",
    "categories": "llms, genai, study, AIML, statisticcs",
    "tags": "llms, genai, study, AIML, statisticcs",
    "date": "2025-07-09 02:20:00 +0530",
    





    
    "snippet": "YouTube is Free University for AI/ML EngineersThese 10 channels will teach you more than any $5k bootcamp:      Andrej Karpathy: Watch hereFormer OpenAI researcher teaches you to implement transfor...",
    "content": "YouTube is Free University for AI/ML EngineersThese 10 channels will teach you more than any $5k bootcamp:      Andrej Karpathy: Watch hereFormer OpenAI researcher teaches you to implement transformers, backpropagation, and neural networks line by line.        3Blue1Brown: Watch hereVisual explanations of math behind machine learning. Makes complex concepts like backpropagation and linear algebra intuitive with beautiful animations.        StatQuest with Josh Starmer: Watch here10-15 minute videos explaining one ML concept each. Covers decision trees, random forests, SVMs, and regression with simple examples and clear diagrams.        sentdex: Watch hereComplete Python tutorials for real AI projects by Harrison Kinsley. Build trading bots, sentiment analysis tools, and computer vision apps with step-by-step coding walkthroughs.        Jeremy Howard: Watch herePractical deep learning course focused on getting results fast. Learn to build image classifiers and text models without getting lost in theory.        Krish Naik: Watch hereEnd-to-end ML project tutorials covering data cleaning to deployment. Shows you complete workflows using real datasets and industry tools.        MIT OpenCourseWare: Watch hereFull university-level AI courses from top professors. Access the same lectures on reinforcement learning and computer vision that MIT students get.        Serrano Academy: Watch hereComplex AI concepts explained with simple diagrams by Luis Serrano. Perfect for understanding transformers, attention mechanisms, and modern architectures visually.        DeepLearningAI: Watch hereAndrew Ng’s channel offering professional courses on ML/DL fundamentals. Features interviews with AI leaders.        Stanford Online: Watch hereLatest AI research papers explained in plain English. Stay updated with what’s happening in top AI labs.  "
  },
  
  {
    "title": "Spegel - Stateless Local OCI Mirror",
    "url": "/posts/Spegel-stateless-local-OCI-mirror/",
    "categories": "OCI, Mirror, Spegel",
    "tags": "OCI, Mirror, Spegel",
    "date": "2024-11-04 00:00:00 +0530",
    





    
    "snippet": "Spegel describes itself as a stateless, cluster-local OCI registry mirror. In this post, we’ll decipher what that means and explore why Spegel might be valuable for your Kubernetes (K8s) setup.What...",
    "content": "Spegel describes itself as a stateless, cluster-local OCI registry mirror. In this post, we’ll decipher what that means and explore why Spegel might be valuable for your Kubernetes (K8s) setup.What is Spegel?  Stateless: Spegel doesn’t store image manifests or layers itself; it relies on containerd’s store on the node.  Local: It operates within your local K8s cluster.  OCI Registry: It’s compliant with the OCI distribution specification.  Mirror: It mirrors images from a remote registry to your local cluster. (Fun fact: “Spegel” means mirror in Swedish.)Why Use Spegel?  Reduce Bandwidth Usage to remote registries.  Faster Image Pulls if images are cached within the K8s cluster.  Fault Tolerance if the remote registry is unavailable.How Spegel WorksThis blog focuses on visual diagrams that illustrate Spegel’s inner workings, making it easier to map component connections and debug issues.Spegel OverviewNodes in the cluster that need an image will first check if it’s available locally.  If available, they pull it directly from other nodes using the P2P network.  If not available, the system falls back to the remote registry.Spegel: Visual Architecture GuideLet’s dive into Spegel’s architecture through a series of diagrams:1. High-Level Cluster ArchitectureThis diagram shows how Spegel pods form a P2P network within the cluster. Each Spegel pod interacts with containerd and, if necessary, falls back to the external registry.Cluster Architecture2. Pod Component ArchitectureDisplays the components within a Spegel pod, including registry services, P2P components, and state management.Pod Component Architecture3. Image Pull FlowThis sequence shows how an image pull request is handled, covering both peer pulls and fallback to external registry.Image Pull Flow4. P2P Network FormationIllustrates how nodes discover each other and form the P2P network through leader election and peer sharing.P2P Network Formation5. State Management and Content AdvertisementDepicts how content availability is maintained and advertised across the P2P network.State Management and Content Advertisement6. Content Resolution ProcessContent Resolution Process7. Data Flow PathsDescribes content and control flow within the system, including peer transfers and fallback.Data Flow Paths8. Failure HandlingDemonstrates failure handling scenarios within the system.Failure Handling9. Metrics CollectionShows how metrics are collected and organized across the system components.Metrics CollectionConclusionI hope I gave a visual understanding of the Spegel project and its architecture. If you got intrigued, next step would be to dive into the source code at Spegel GitHub."
  },
  
  {
    "title": "How to Efficiently Serve an LLM?",
    "url": "/posts/How-to-Efficiently-serve-an-llm/",
    "categories": "LLM, Inference, Optimization, Serving",
    "tags": "LLM, inference, optimization, serving",
    "date": "2024-08-05 05:30:00 +0530",
    





    
    "snippet": "How to Efficiently Serve an LLMLLMs, or Large Language Models, are named so because they can range from tens to hundreds of billions of parameters. Their utility is clear, as LLMs are setting new r...",
    "content": "How to Efficiently Serve an LLMLLMs, or Large Language Models, are named so because they can range from tens to hundreds of billions of parameters. Their utility is clear, as LLMs are setting new records on various benchmarks and now often match or exceed human performance in multiple tasks GPT-4 Technical Report. Consequently, many companies are eager to deploy them in production. However, due to the unprecedented size of LLMs, there are significant challenges in serving them, such as slow token generation (tokens/second), memory limits for loading model parameters, KV cache (explained later), compute limits, and more. In this article, we will cover several recent ideas to help set up a robust LLM serving system.LLM Serving OverviewLLM inference Steps  Multiple users send requests to the LLM Server through HTTPs/gRPC.  The LLM Server receives the requests and schedules them based on QoE definition.          QoE (Quality of Experience): Defined by two metrics:                  TTFT (Time to First Token): The time it takes for a user to receive the first response token.          TDS (Token Delivery Speed): The rate at which the user receives tokens, which should be uniform and above the reader’s reading speed for a positive user experience.                      QoE Aware LLM Serving    After scheduling, the LLM Inference process is divided into two phases:                  Prefill phase: The LLM processes the input tokens in parallel and generates the output activations known as the “KV Cache”. This step is highly efficient at utilizing the GPU’s parallel processing capabilities, making input tokens generally much cheaper than output tokens (as seen in the GPT-4o pricing chart). This phase produces the first output token and is typically compute-bound.                    GPT-4o Pricing                    Decode phase: The LLM starts autoregressively generating output tokens one at a time. This phase is slower in terms of inference and is where optimizations are necessary. Output tokens at each step are concatenated with the previous tokens’ KV cache to generate the next token.                    KV Cache Explanation &amp; Reuse            OptimizationsMany experts are innovating the inference stack, and multiple startups are competing to reduce costs to attract more customers.  LLAMA 405 Pricing by Different ProvidersHere are some interesting optimizations shared recently in research:  Batching:          Instead of serving one request at a time and wasting compute resources (since the decode phase has low arithmetic intensity and is memory-bound), we can amortize the cost of retrieving weights and KV cache from memory by serving multiple requests simultaneously.      Continuous Batching        Model Quantization (FP8/INT8):          Decreasing the precision of model weights and/or activations (AWQ/GPTQ) frees up more GPU VRAM, which allows for serving larger batches of requests.      Model Quantization        Paged Attention:          The core idea behind vLLM, the most popular open-source serving engine, is to avoid memory fragmentation that occurs due to preserving the max context length for every request by using paging (borrowed from OS paging) to manage memory efficiently.      Paged Attention in vLLM        Prefill Chunking / Stale-free Batching:          Proposed by the Sarathi-Serve paper, dividing the prefill context into smaller chunks allows merging the prefill and decode phases of different requests in the same batch.      Prefill Decode Prioritizing        Prefill/Decode Disaggregation:          In constrast to the previous idea, this paper Mooncake: A KVCache-centric DisaggregatedArchitecture for LLM Serving proposes separating the prefill and decode phases and transferring KVCache through a specialized design.      KVCache Transfer in Disaggregated Architecture        KVCache Compression:          As proposed by CacheGen, compressing the KVCache to speed up network transfer. This approach is beneficial for use cases with large context lengths (i.e., content summarization) which are over 16k input tokens to justify the encoding/decoding CPU overhead.      KV Cache Compression        Speculative Decoding:          Using extra smaller model(s) that generate tokens fast and in parallel. Selecting the output that matches the original model can speed up inference for simple use cases. Note that as the request batch size increases, the speed-up of speculative decoding diminishes.      Speculative Decoding        Radix Attention (Prefix Caching):          This is the idea behind SGLang (SGLang: Efficient Execution ofStructured Language Model Programs), which involves creating a data structure similar to a Prefix tree (Trie) for the KVCache to help reuse KVCache without recomputation. This only works for some use cases, like those shown in the image below:      KV Cache Sharing Examples        Early Rejection:          Predicting if a request can be served once received to avoid wasted resources (i.e., the server successfully computed the prefill part but failed at the decode phase due to memory limitations) will help improve server resource utilization and prevent downtime.      Early Rejection based on Prediction      ConclusionEfficiently serving large language models is essential for businesses to reduce costs and increase generation speed (tokens/second). This efficiency opens the door for more use cases for LLMs. With the ideas presented here, you can optimize your LLM inference stack to achieve these goals and more!References  Improving LLM Inference with Prefill Chunking / Stale-free batching (USENIX)  Mooncake: A KVCache-centric DisaggregatedArchitecture for LLM Serving  KVCache Compression and Streaming for Faster LLM Serving (arXiv)  Dynamic Memory Management for LLMs: vAttention (arXiv)  Enhancing Quality-of-Experience in LLM-Based Services (arXiv)  Prefix Caching for Efficient LLM Inference (arXiv)  Mastering LLM Techniques: Inference Optimization (NVIDIA Technical Blog)  Token Probability Distribution (Hugging Face)  Welcome to vLLM! — vLLM Documentation  Serving Large Language Models: Technologies and Choices (run.ai)  Efficient Large Language Model Serving (arXiv)"
  },
  
  {
    "title": "The Tech Behind TikTok's Addictive Recommendation System",
    "url": "/posts/the-tech-behind-tiktoks-addicitve-recommendation-system/",
    "categories": "TikTok, Recommendation Systems, Kafka, Flink",
    "tags": "tiktok, recommendation systems, kafka, flink",
    "date": "2023-12-05 01:00:00 +0530",
    





    
    "snippet": "IntroI’ve been using TikTok a lot lately and was curious about the tech behind TikTok’s addictive algorithm, the “Recommendation System.” I visited their official blog post but found limited detail...",
    "content": "IntroI’ve been using TikTok a lot lately and was curious about the tech behind TikTok’s addictive algorithm, the “Recommendation System.” I visited their official blog post but found limited details on how it works. So, I googled and found two papers “Monolith” and “IPS,” released by ByteDance, TikTok’s parent company, and here is the process as explained by them.Recommendation featuresThere are features that influence the recommendation system, which are:User Features:  User interactions, such as the videos you like, share, comment on, watch in full or skip, accounts you follow, accounts that follow you, and when you create content.  User information, such as your device and account settings, language preference, country, time zone and day, and device type.Content Features:  Video information, such as captions, sounds, hashtags, number of video views, and the country in which the video was published.These elements are merged to create detailed user profiles and content embeddings. These profiles and embeddings are then used to tailor content suggestions using a mix of collaborative and content-based approaches.Model architectureEmbeddings are input to a Deep Factorization Machine, which leverages a deep neural network to capture non-linear and complex patterns from the embeddings, and Factorization Machines to capture linear correlations between different features, which is essential in understanding simple yet effective user-item interactions.Model ArchitectureReal-Time Online TrainingTikTok has to update its recommendation system as fast as possible to account for the non-stationary nature of user data, known as “Concept Drift.” So, there has to be a mechanism that updates the model parameters in real-time.“Monolith” framework uses a streaming engine that can be used in both batch and online training in a unified design. They use a Kafka queue to log actions of users (clicks, likes, comments, etc.) and another Kafka queue for features. And Flink as a streaming engine/job for joining them to create training examples.During online training, sparse parameters are updated on a minute-level interval from the training PS (Parameter Synchronization) to the serving PS, which avoids heavy network transmissions or memory spikes.Training PipelineMonolith uses TensorFlow’s distributed Worker-ParameterServer, where multiple machines work together to train a model. Workers are responsible for performing computations (gradients/parameter updates), while parameter servers are used for storing the current model state like weights and biases.Managing Large and Dynamic User Data: Embedding CollisionsUser data’s vast and dynamic nature poses a challenge, as it can lead to unwieldy embedding sizes and collisions (where different data points are mistakenly identified as identical).To tackle this, “Monolith” employs “cuckoo hashing.” This method uses two hash tables with separate hash functions, allowing dynamic placement and repositioning of elements to avoid collisions.Additionally, to prevent the embedding memory from expanding too rapidly, a probabilistic filter and an adjustable expiration period for embeddings are used, effectively managing the memory requirements.ConclusionTikTok’s recommendation system played a main role in its success and widespread use. I tried in this blog post to shed some light on the underlying technologies used, especially the online training, which helps them recommend real-time personalized content that keeps users staring at their phones for hours.References:  How TikTok recommends content  Monolith: Real Time Recommendation System With Collisionless Embedding Table (arxiv.org)  *An Empirical Investigation of Personalization Factors on TikTok (arxiv.org)  cs.princeton.edu/courses/archive/spring21/cos598D/icde_2021_camera_ready.pdf"
  },
  
  {
    "title": "Do you really need a Vector Database?",
    "url": "/posts/do-you-really-need-a-vector-database/",
    "categories": "LLMs, GenAI, VectorDBs, Embeddings",
    "tags": "llms, genai, vectordbs, embeddings",
    "date": "2023-11-22 02:20:00 +0530",
    





    
    "snippet": "Do You Really Need a VectorDB?Everywhere I go, I see his face – “VectorDBs”. Seriously, with the rise of GenAI and the growing number of applications utilizing Large Language Models (LLMs), Vector ...",
    "content": "Do You Really Need a VectorDB?Everywhere I go, I see his face – “VectorDBs”. Seriously, with the rise of GenAI and the growing number of applications utilizing Large Language Models (LLMs), Vector Databases have been posited as a de facto component in any LLM-powered application architecture. You might wonder why. The answer is seemingly straightforward: “LLMs have a limited context window, and to select the most relevant data, a VectorDB is necessary.” This seems logical at first, but let’s think more about it and consider the alternatives.Spiderman meme of seeing vector databases everywhere1. Why Vector Search over Keyword Search?Although Vector Search is popular for its ability to deliver semantically similar results, something Keyword Search can’t do, it’s not without its drawbacks. Keyword search is more accurate, faster, and can scale with new data more effectively.Keyword search excels in finding exact matches and can be easily updated with synonyms and taxonomies. In contrast, Vector search requires a vectorizer (embedding model) to process every query, which can incur additional costs and latency. It’s also susceptible to data drift, as the embedding model might become outdated, missing new terminology and relationships.In essence, keyword search is more interpretable, cost-effective, and often provides better performance.2. Why Vector Database over a Vector Index?If you still think Vector Search is more suitable for your case, consider using a Vector Index like Facebook’s “FAISS,” which supports the most popular indexing algorithms like “HNSW” and various similarity measures such as “L2 distance”, “dot products”, and cosine similarity. It can also handle indices that don’t fit in-memory.3. Why Vector Database over a General-Purpose Database?Databases like PostgreSQL, with its pgvector extension, and Elasticsearch, with dense vector indexing, support vector storage, indexing, and similarity search capabilities, along with traditional database benefits like ACID compliance, point-in-time recovery, joins, etc.But,Vector Databases definitely have their specialized use cases, especially for users managing billions of data embeddings. They benefit from architecture decisions that can improve performance over general-purpose databases, ensuring efficient storage of vectors and fast retrieval for similarity search calculations, while offering data safety. However, they aren’t the universal solution they’re often portrayed as.Further Readings:  Beware Tunnel Vision in AI Retrieval - by Colin Harman  Do you actually need a vector database? - Ethan Rosenthal#vectordbs #llms #genai #ir"
  }
  
]


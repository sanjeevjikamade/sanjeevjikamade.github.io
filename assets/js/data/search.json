[
  
  {
    "title": "Spegel - Stateless Local OCI Mirror",
    "url": "/posts/Spegel-stateless-local-OCI-mirror/",
    "categories": "OCI, Mirror, Spegel",
    "tags": "OCI, Mirror, Spegel",
    "date": "2024-11-04 00:00:00 +0530",
    





    
    "snippet": "Spegel describes itself as a stateless, cluster-local OCI registry mirror. In this post, we’ll decipher what that means and explore why Spegel might be valuable for your Kubernetes (K8s) setup.What...",
    "content": "Spegel describes itself as a stateless, cluster-local OCI registry mirror. In this post, we’ll decipher what that means and explore why Spegel might be valuable for your Kubernetes (K8s) setup.What is Spegel?  Stateless: Spegel doesn’t store image manifests or layers itself; it relies on containerd’s store on the node.  Local: It operates within your local K8s cluster.  OCI Registry: It’s compliant with the OCI distribution specification.  Mirror: It mirrors images from a remote registry to your local cluster. (Fun fact: “Spegel” means mirror in Swedish.)Why Use Spegel?  Reduce Bandwidth Usage to remote registries.  Faster Image Pulls if images are cached within the K8s cluster.  Fault Tolerance if the remote registry is unavailable.How Spegel WorksThis blog focuses on visual diagrams that illustrate Spegel’s inner workings, making it easier to map component connections and debug issues.Spegel OverviewNodes in the cluster that need an image will first check if it’s available locally.  If available, they pull it directly from other nodes using the P2P network.  If not available, the system falls back to the remote registry.Spegel: Visual Architecture GuideLet’s dive into Spegel’s architecture through a series of diagrams:1. High-Level Cluster ArchitectureThis diagram shows how Spegel pods form a P2P network within the cluster. Each Spegel pod interacts with containerd and, if necessary, falls back to the external registry.Cluster Architecture2. Pod Component ArchitectureDisplays the components within a Spegel pod, including registry services, P2P components, and state management.Pod Component Architecture3. Image Pull FlowThis sequence shows how an image pull request is handled, covering both peer pulls and fallback to external registry.Image Pull Flow4. P2P Network FormationIllustrates how nodes discover each other and form the P2P network through leader election and peer sharing.P2P Network Formation5. State Management and Content AdvertisementDepicts how content availability is maintained and advertised across the P2P network.State Management and Content Advertisement6. Content Resolution ProcessContent Resolution Process7. Data Flow PathsDescribes content and control flow within the system, including peer transfers and fallback.Data Flow Paths8. Failure HandlingDemonstrates failure handling scenarios within the system.Failure Handling9. Metrics CollectionShows how metrics are collected and organized across the system components.Metrics CollectionConclusionI hope I gave a visual understanding of the Spegel project and its architecture. If you got intrigued, next step would be to dive into the source code at Spegel GitHub."
  },
  
  {
    "title": "How to Efficiently Serve an LLM?",
    "url": "/posts/How-to-Efficiently-serve-an-llm/",
    "categories": "LLM, Inference, Optimization, Serving",
    "tags": "LLM, inference, optimization, serving",
    "date": "2024-08-05 05:30:00 +0530",
    





    
    "snippet": "How to Efficiently Serve an LLMLLMs, or Large Language Models, are named so because they can range from tens to hundreds of billions of parameters. Their utility is clear, as LLMs are setting new r...",
    "content": "How to Efficiently Serve an LLMLLMs, or Large Language Models, are named so because they can range from tens to hundreds of billions of parameters. Their utility is clear, as LLMs are setting new records on various benchmarks and now often match or exceed human performance in multiple tasks GPT-4 Technical Report. Consequently, many companies are eager to deploy them in production. However, due to the unprecedented size of LLMs, there are significant challenges in serving them, such as slow token generation (tokens/second), memory limits for loading model parameters, KV cache (explained later), compute limits, and more. In this article, we will cover several recent ideas to help set up a robust LLM serving system.LLM Serving OverviewLLM inference Steps  Multiple users send requests to the LLM Server through HTTPs/gRPC.  The LLM Server receives the requests and schedules them based on QoE definition.          QoE (Quality of Experience): Defined by two metrics:                  TTFT (Time to First Token): The time it takes for a user to receive the first response token.          TDS (Token Delivery Speed): The rate at which the user receives tokens, which should be uniform and above the reader’s reading speed for a positive user experience.                      QoE Aware LLM Serving    After scheduling, the LLM Inference process is divided into two phases:                  Prefill phase: The LLM processes the input tokens in parallel and generates the output activations known as the “KV Cache”. This step is highly efficient at utilizing the GPU’s parallel processing capabilities, making input tokens generally much cheaper than output tokens (as seen in the GPT-4o pricing chart). This phase produces the first output token and is typically compute-bound.                    GPT-4o Pricing                    Decode phase: The LLM starts autoregressively generating output tokens one at a time. This phase is slower in terms of inference and is where optimizations are necessary. Output tokens at each step are concatenated with the previous tokens’ KV cache to generate the next token.                    KV Cache Explanation &amp; Reuse            OptimizationsMany experts are innovating the inference stack, and multiple startups are competing to reduce costs to attract more customers.  LLAMA 405 Pricing by Different ProvidersHere are some interesting optimizations shared recently in research:  Batching:          Instead of serving one request at a time and wasting compute resources (since the decode phase has low arithmetic intensity and is memory-bound), we can amortize the cost of retrieving weights and KV cache from memory by serving multiple requests simultaneously.      Continuous Batching        Model Quantization (FP8/INT8):          Decreasing the precision of model weights and/or activations (AWQ/GPTQ) frees up more GPU VRAM, which allows for serving larger batches of requests.      Model Quantization        Paged Attention:          The core idea behind vLLM, the most popular open-source serving engine, is to avoid memory fragmentation that occurs due to preserving the max context length for every request by using paging (borrowed from OS paging) to manage memory efficiently.      Paged Attention in vLLM        Prefill Chunking / Stale-free Batching:          Proposed by the Sarathi-Serve paper, dividing the prefill context into smaller chunks allows merging the prefill and decode phases of different requests in the same batch.      Prefill Decode Prioritizing        Prefill/Decode Disaggregation:          In constrast to the previous idea, this paper Mooncake: A KVCache-centric DisaggregatedArchitecture for LLM Serving proposes separating the prefill and decode phases and transferring KVCache through a specialized design.      KVCache Transfer in Disaggregated Architecture        KVCache Compression:          As proposed by CacheGen, compressing the KVCache to speed up network transfer. This approach is beneficial for use cases with large context lengths (i.e., content summarization) which are over 16k input tokens to justify the encoding/decoding CPU overhead.      KV Cache Compression        Speculative Decoding:          Using extra smaller model(s) that generate tokens fast and in parallel. Selecting the output that matches the original model can speed up inference for simple use cases. Note that as the request batch size increases, the speed-up of speculative decoding diminishes.      Speculative Decoding        Radix Attention (Prefix Caching):          This is the idea behind SGLang (SGLang: Efficient Execution ofStructured Language Model Programs), which involves creating a data structure similar to a Prefix tree (Trie) for the KVCache to help reuse KVCache without recomputation. This only works for some use cases, like those shown in the image below:      KV Cache Sharing Examples        Early Rejection:          Predicting if a request can be served once received to avoid wasted resources (i.e., the server successfully computed the prefill part but failed at the decode phase due to memory limitations) will help improve server resource utilization and prevent downtime.      Early Rejection based on Prediction      ConclusionEfficiently serving large language models is essential for businesses to reduce costs and increase generation speed (tokens/second). This efficiency opens the door for more use cases for LLMs. With the ideas presented here, you can optimize your LLM inference stack to achieve these goals and more!References  Improving LLM Inference with Prefill Chunking / Stale-free batching (USENIX)  Mooncake: A KVCache-centric DisaggregatedArchitecture for LLM Serving  KVCache Compression and Streaming for Faster LLM Serving (arXiv)  Dynamic Memory Management for LLMs: vAttention (arXiv)  Enhancing Quality-of-Experience in LLM-Based Services (arXiv)  Prefix Caching for Efficient LLM Inference (arXiv)  Mastering LLM Techniques: Inference Optimization (NVIDIA Technical Blog)  Token Probability Distribution (Hugging Face)  Welcome to vLLM! — vLLM Documentation  Serving Large Language Models: Technologies and Choices (run.ai)  Efficient Large Language Model Serving (arXiv)"
  },
  
  {
    "title": "The Tech Behind TikTok's Addictive Recommendation System",
    "url": "/posts/the-tech-behind-tiktoks-addicitve-recommendation-system/",
    "categories": "TikTok, Recommendation Systems, Kafka, Flink",
    "tags": "tiktok, recommendation systems, kafka, flink",
    "date": "2023-12-05 01:00:00 +0530",
    





    
    "snippet": "IntroI’ve been using TikTok a lot lately and was curious about the tech behind TikTok’s addictive algorithm, the “Recommendation System.” I visited their official blog post but found limited detail...",
    "content": "IntroI’ve been using TikTok a lot lately and was curious about the tech behind TikTok’s addictive algorithm, the “Recommendation System.” I visited their official blog post but found limited details on how it works. So, I googled and found two papers “Monolith” and “IPS,” released by ByteDance, TikTok’s parent company, and here is the process as explained by them.Recommendation featuresThere are features that influence the recommendation system, which are:User Features:  User interactions, such as the videos you like, share, comment on, watch in full or skip, accounts you follow, accounts that follow you, and when you create content.  User information, such as your device and account settings, language preference, country, time zone and day, and device type.Content Features:  Video information, such as captions, sounds, hashtags, number of video views, and the country in which the video was published.These elements are merged to create detailed user profiles and content embeddings. These profiles and embeddings are then used to tailor content suggestions using a mix of collaborative and content-based approaches.Model architectureEmbeddings are input to a Deep Factorization Machine, which leverages a deep neural network to capture non-linear and complex patterns from the embeddings, and Factorization Machines to capture linear correlations between different features, which is essential in understanding simple yet effective user-item interactions.Model ArchitectureReal-Time Online TrainingTikTok has to update its recommendation system as fast as possible to account for the non-stationary nature of user data, known as “Concept Drift.” So, there has to be a mechanism that updates the model parameters in real-time.“Monolith” framework uses a streaming engine that can be used in both batch and online training in a unified design. They use a Kafka queue to log actions of users (clicks, likes, comments, etc.) and another Kafka queue for features. And Flink as a streaming engine/job for joining them to create training examples.During online training, sparse parameters are updated on a minute-level interval from the training PS (Parameter Synchronization) to the serving PS, which avoids heavy network transmissions or memory spikes.Training PipelineMonolith uses TensorFlow’s distributed Worker-ParameterServer, where multiple machines work together to train a model. Workers are responsible for performing computations (gradients/parameter updates), while parameter servers are used for storing the current model state like weights and biases.Managing Large and Dynamic User Data: Embedding CollisionsUser data’s vast and dynamic nature poses a challenge, as it can lead to unwieldy embedding sizes and collisions (where different data points are mistakenly identified as identical).To tackle this, “Monolith” employs “cuckoo hashing.” This method uses two hash tables with separate hash functions, allowing dynamic placement and repositioning of elements to avoid collisions.Additionally, to prevent the embedding memory from expanding too rapidly, a probabilistic filter and an adjustable expiration period for embeddings are used, effectively managing the memory requirements.ConclusionTikTok’s recommendation system played a main role in its success and widespread use. I tried in this blog post to shed some light on the underlying technologies used, especially the online training, which helps them recommend real-time personalized content that keeps users staring at their phones for hours.References:  How TikTok recommends content  Monolith: Real Time Recommendation System With Collisionless Embedding Table (arxiv.org)  *An Empirical Investigation of Personalization Factors on TikTok (arxiv.org)  cs.princeton.edu/courses/archive/spring21/cos598D/icde_2021_camera_ready.pdf"
  },
  
  {
    "title": "Do you really need a Vector Database?",
    "url": "/posts/do-you-really-need-a-vector-database/",
    "categories": "LLMs, GenAI, VectorDBs, Embeddings",
    "tags": "llms, genai, vectordbs, embeddings",
    "date": "2023-11-22 02:20:00 +0530",
    





    
    "snippet": "Do You Really Need a VectorDB?Everywhere I go, I see his face – “VectorDBs”. Seriously, with the rise of GenAI and the growing number of applications utilizing Large Language Models (LLMs), Vector ...",
    "content": "Do You Really Need a VectorDB?Everywhere I go, I see his face – “VectorDBs”. Seriously, with the rise of GenAI and the growing number of applications utilizing Large Language Models (LLMs), Vector Databases have been posited as a de facto component in any LLM-powered application architecture. You might wonder why. The answer is seemingly straightforward: “LLMs have a limited context window, and to select the most relevant data, a VectorDB is necessary.” This seems logical at first, but let’s think more about it and consider the alternatives.Spiderman meme of seeing vector databases everywhere1. Why Vector Search over Keyword Search?Although Vector Search is popular for its ability to deliver semantically similar results, something Keyword Search can’t do, it’s not without its drawbacks. Keyword search is more accurate, faster, and can scale with new data more effectively.Keyword search excels in finding exact matches and can be easily updated with synonyms and taxonomies. In contrast, Vector search requires a vectorizer (embedding model) to process every query, which can incur additional costs and latency. It’s also susceptible to data drift, as the embedding model might become outdated, missing new terminology and relationships.In essence, keyword search is more interpretable, cost-effective, and often provides better performance.2. Why Vector Database over a Vector Index?If you still think Vector Search is more suitable for your case, consider using a Vector Index like Facebook’s “FAISS,” which supports the most popular indexing algorithms like “HNSW” and various similarity measures such as “L2 distance”, “dot products”, and cosine similarity. It can also handle indices that don’t fit in-memory.3. Why Vector Database over a General-Purpose Database?Databases like PostgreSQL, with its pgvector extension, and Elasticsearch, with dense vector indexing, support vector storage, indexing, and similarity search capabilities, along with traditional database benefits like ACID compliance, point-in-time recovery, joins, etc.But,Vector Databases definitely have their specialized use cases, especially for users managing billions of data embeddings. They benefit from architecture decisions that can improve performance over general-purpose databases, ensuring efficient storage of vectors and fast retrieval for similarity search calculations, while offering data safety. However, they aren’t the universal solution they’re often portrayed as.Further Readings:  Beware Tunnel Vision in AI Retrieval - by Colin Harman  Do you actually need a vector database? - Ethan Rosenthal#vectordbs #llms #genai #ir"
  },
  
  {
    "title": "Starting A Blog Hosted On Github Pages",
    "url": "/posts/how-to-build-website/",
    "categories": "Blogging, Tutorial",
    "tags": "github-pages, blog, personal blog, jekyll",
    "date": "2023-06-01 22:44:00 +0530",
    





    
    "snippet": "My first ever blog postI’ve been thinking about starting a blog for a while now and I was procrastinating quite a bit :D. But, I finally did it and here I am writing my first ever blog post. Supris...",
    "content": "My first ever blog postI’ve been thinking about starting a blog for a while now and I was procrastinating quite a bit :D. But, I finally did it and here I am writing my first ever blog post. Suprisingly enough, it will be about my experience setting up my blog and how you can do it too :D.Scenario of a developer starting a blogYouTube Video WalkthroughWhy Github Pages?I’m a progammer. I’ve always wanted to have a personal website to showcase my projects and share my thoughts. I’ve looked into various blogging platforms like Wordpress, Medium, Substack, and Ghost. But, I chose Github Pages with Jekyll because I wanted to have:  Full control over my blog and I wanted to customize it to my taste.  A blog that is free and doesn’t require me to pay for hosting.  A blog that is simple, fast and easy to maintain doesn’t require me to spend hours to configure it.Did I convince you? OK, now let’s break down the steps to setup your blogging site.Step 1: Decide Your ThemeThis step is to quickly browse through the various Jekyll themes available on various websites and pick one that fits your tasteFew sites where you can grab these templates:  https://jekyllthemes.io/  http://jekyllthemes.org/  https://jekyll-themes.com/  https://jamstackthemes.dev/ssg/jekyll/I personally picked the Chirpy theme since it fits my expectations and it has a Dark theme :D.Step 2: Activate Github PagesOnce you pick the Jekyll theme, it’s time to host it on Github Pages. The theme you picked usually comes with a set of instructions to configure and the instruction varies between different themes.For Chirpy theme, the instructions are as follows:  Use the template to create your own repository.          Make sure to name it as &lt;your-gh-username&gt;.github.io      After doing this step Github actions will build and deploy your blog automatically to &lt;your-gh-username&gt;.github.io      But you don’t want just a template, you want to customized it to yourself. So, let’s move on to the next step.        Clone the repository you just created.  Install Ruby and Jekyll on your machine through the official guide.  Run bundle install to install the required gems.  Update the variables of _config.yml as needed. Some of them are typical options.          url is the address of your website      avatar is the profile picture in the sidebar      timezone is used to display the date and time of your posts      lang is the language of the site        Run bundle exec jekyll s to start the local server.The empty template you should seeIf you face any issues, you can refer to the Chripy theme’s Getting started guide.Step 3: Setup Your Custom Root DomainYou need to visit one of the domain name registrar to buy a custom domain. There are multiple registrars to choose from:  GoDaddy  Google Domains  Name Cheap  … and many moreI personally chose GoDaddy since I had to pay ~10$ for 2-years planConfigure Your DomainAfter you purchase your domain, go into your domain management portal, click on manage DNS and add A type DNS records for github pages.            Type      Data                  A      185.199.108.153              A      185.199.109.153              A      185.199.110.153              A      185.199.111.153              CNAME      gh-username.github.io      (These A type DNS records map your domain name to the Github’s IP address)So far, my DNS record looks like this:My GoDaddy’s DNS recordsConfigure Github PagesNow that you have your domain’s DNS setup, Let’s head back to Github and configure your Github Pages to use your custom domain.  Go to your repository’s settings page.  Scroll down to the Pages section.  Under Custom domain enter your domain name and click Save.My Github Pages Custom Domain pageBest Practice : Click on Enforce HTTPS to serve your blog via secure SSL connection. Your site will be configured with a free SSL certificate from Let’s Encrypt.Bonus Tip: Test Your SiteIf you see a 404 error or Domain Not Found error your DNS record might not be updated. Every time you update a DNS record, it takes few mins to several hours to propagate the WWW. So, give it sometime. To see if your domain is reachable, you could dig the DNS:$ dig YOUR-DOMAIN.COM +noall +answerHere is a reference from digging my website:$ dig ahmedtremo.com +noall +answerahmedtremo.com.    0    IN    A    185.199.111.153ahmedtremo.com.    0    IN    A    185.199.109.153ahmedtremo.com.    0    IN    A    185.199.110.153ahmedtremo.com.    0    IN    A    185.199.108.153Hope you found this article useful. If you have any questions, you can check my blog’s repo on Github or feel free to reach out to me on Twitter or LinkedIn."
  }
  
]

